{
  "config": {
    "lang": ["en"],
    "separator": "[\\s\\-]+",
    "pipeline": ["stopWordFilter"],
    "fields": {
      "title": { "boost": 1000.0 },
      "text": { "boost": 1.0 },
      "tags": { "boost": 1000000.0 }
    }
  },
  "docs": [
    {
      "location": "index.html",
      "title": "About HELLO vjhvjvj",
      "text": "<p>Political Debates is a projects that aims to facilitate analysis of video or audio content in the following ways:</p> <ul> <li>transcripts and translations are AI generated and split content per speakers</li> <li>a user interface is offered to add annotations for the speakers and to correct   the AI generated transcriptions and translations</li> <li>a search engine that is updated with the modified content helps to find   relevant statements of speakers</li> </ul>"
    },
    {
      "location": "index.html#context",
      "title": "Context",
      "text": "<p>The project was developed in the context of analyzing political debate by the UNHCR. But it can be generalized to other video and audio content, where the focus is on speakers that take turns discussing a topic. The input are just video and audio files, with no specific assumptions. The goal is to make the content available per speaker and to be able to search into who said what when.</p>"
    },
    {
      "location": "index.html#user-interface-with-two-roles",
      "title": "User Interface with two roles",
      "text": "<p>The project comes with a user interface and assumes two roles:</p> <ul> <li><code>Editor</code>: can process and admister the data and metadata</li> <li><code>Reader</code>: has access to the processed videos and their metadata, but cannot   edit any data or metadata</li> </ul> <p>Both roles have access to the user interface, but the reader is restricted to certain pages.</p>"
    },
    { "location": "index.html#parts", "title": "Parts", "text": "" },
    {
      "location": "index.html#dashboard",
      "title": "Dashboard",
      "text": "<p>{ width=\"800\" }</p> <ul> <li>only the <code>Editor</code> has access</li> <li>new videos and audio files can be uploaded to start processing</li> <li>all videos and audio files are listed and can be deleted</li> <li>indexing for processed videos and audios can be repeated</li> </ul>"
    },
    {
      "location": "index.html#searchpage",
      "title": "Searchpage",
      "text": "<p>{ width=\"800\" }</p> <ul> <li>both roles have access</li> <li>the search page can be used to search in speaker statements for a term or   filter the statements by certain criteria that have been added as tags to the   segments by the <code>Editor</code>.</li> </ul>"
    },
    {
      "location": "index.html#mediaplayer",
      "title": "Mediaplayer",
      "text": "<p>{ width=\"800\" }</p> <ul> <li>both roles have access</li> <li>the <code>Editor</code> can edit and add metadata on speakers and videos and correct   transcripts and translations</li> <li>the <code>Reader</code> can view and jump around in the media replay via pointing to text   in transcripts and translations</li> </ul>"
    },
    {
      "location": "index.html#poc",
      "title": "PoC",
      "text": "<p>The project has been done as a Proof of Concept: it is not production ready in the following ways:</p> <ul> <li>the data is stored on docker volumes. The volumes are limited by the diskspace   on the server and their are no data recovery strategies in place</li> <li>the authentication uses nginx basic auth to distinguish between just two   roles. There is no real user authentication in place besides these teo roles.</li> </ul>"
    },
    { "location": "architecture.html", "title": "Architecture", "text": "" },
    {
      "location": "architecture.html#architecture",
      "title": "Architecture",
      "text": "<p>The application consists of the following parts:</p> <ul> <li> <p>Frontend: in svelte kit with a generated API client (generated from the   backend)</p> </li> <li> <p>Backend: in python with a FastAPI interface: all actions happen here and   can be started either via the frontend or per API</p> </li> <li> <p>Workers: via redis queue: to decouple the time consuming processing from   access requests on the data and metadata a redis queue has been established to   pipe the uploaded media files through a pipeline of workers, that convert   videos to audio, derive the transcripts and translations via an external   interface to an AI tool, stored on Hugging Face. After processing the data is   stored on an S3 and then loaded into secondary data storages such as Mongo DB   and Solr</p> </li> <li> <p>Whisper: the external Whisper component that is accessed via API to do the   hard work of transcribing the videos is hosted on Hugging Face. A personal   Hugging Face account and token is needed to gain access for the processing of   the media files</p> </li> </ul> <pre><code>graph TD\n    subgraph Clients [\"User Interaction\"]\n        FE[Frontend&lt;br&gt;SvelteKit + Generated Client]\n    end\n\n    subgraph App_Layer [\"Application Core\"]\n        BE[Backend&lt;br&gt;Python FastAPI]\n    end\n\n    subgraph Async_Layer [\"Asynchronous Processing: Workers\"]\n        Redis[Redis Queue]\n        subgraph Workers1 [\"Convert\"]\n        end\n        subgraph Workers2 [\"Transcribe\"]\n        end\n        subgraph Workers3 [\"Reindex\"]\n        end\n    end\n\n    subgraph External [\"External AI Services\"]\n        Whisper[Whisper Model&lt;br&gt;Hugging Face]\n    end\n\n    subgraph Storage1 [\"Primary Data Storage\"]\n        S3[S3 Storage&lt;br&gt;Media &amp; Artifacts]\n    end\n\n    subgraph Storage2 [\"Secondary Datastorage\"]\n        Mongo[MongoDB]\n        Solr[Solr Search]\n    end\n\n    %% Define Relationships\n    FE --&gt;|API Requests| BE\n\n    BE --&gt;|Enqueue Jobs| Redis\n    BE --&gt;|Load Data    | S3\n    BE --&gt;|Load Data    | Storage2\n\n\n    Workers1 --&gt;|1. Convert Video to Audio| Workers2\n    Workers2 --&gt;|2. Transcription| Whisper\n    Workers3 --&gt;|3. Index Metadata | Storage2\n\n    S3 -.-&gt;|Load Data| Mongo\n    S3 -.-&gt;|Load Data| Solr\n\n    %% Styling\n    classDef primary fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n    classDef secondary fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;\n    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px;\n    classDef external fill:#fff3e0,stroke:#e65100,stroke-width:2px,stroke-dasharray: 5 5;\n\n    class FE,API_User,BE primary;\n    class Redis,Worker secondary;\n    class S3,Mongo,Solr,Secondary_DBs storage;\n    class Whisper external;</code></pre> <ul> <li>loading results into secondary databases: the processed   data is loaded into structured databases to improve findability</li> </ul> <pre><code>flowchart LR\n  S[(Object Store S3)] --load--&gt; E{SRT Parser}\n  E --metadata--&gt; B[(Structured Metadata MongoDB)]\n  E --segments--&gt; C[(Search Engine Solr)]</code></pre> <ul> <li>serving and enriching metadata via a WebUI: the Webui allows   to search in the debates and to annotate and correct the speaker statements</li> </ul> <pre><code>flowchart LR\n    subgraph Backend[Debates Backend]\n        M{Debates API}\n        C[(Search Engine Solr)]\n        S[(Object Store S3)]\n        B[(Structured Metadata MongoDB)]\n    end\n    subgraph UI[Debates GUI]\n        F(GUI Searchpage)\n        E(GUI Videoplayer)\n    end\n    M -- serve --&gt; F\n    M -- serve --&gt; E\n    C -- load --&gt; M\n    B -- load --&gt; M\n    S -- signed urls --&gt; M\n    E -- correct --&gt; M\n    style UI fill:white\n    style Backend fill:white</code></pre>"
    },
    { "location": "dataloader.html", "title": "Dataloader", "text": "" },
    {
      "location": "dataloader.html#overview",
      "title": "Overview",
      "text": "<p>Note</p> <p>Loading the data into the App assumes that you have the backend running: See Install options for setup options</p> <p>The processed data is loaded from Pipeline S3 into 3 databases:</p> <ul> <li>App S3: S3 Database for processed data: this is done manually with only a   small modification see below</li> <li>App MongoDB Mongo database for all metadata on speakers, segments,   transcripts and translations</li> <li>App Solr Solr search engine where speaker segments are loaded as documents   into Solr</li> </ul> <pre><code>flowchart LR\n    subgraph WebApp[Web Application]\n        B[(App S3)]\n        E{SRT Parser}\n        C[(App MongoDB)]\n        D[(App Solr)]\n    end\n    A[(Pipeline S3)] -- manual --&gt; B\n    B --&gt; E\n    E --&gt; C\n    E --&gt; D\n    style WebApp fill:white</code></pre>"
    },
    {
      "location": "dataloader.html#loading-into-the-app-s3",
      "title": "Loading into the App S3",
      "text": "<p>All files are from Pipeline S3 loaded into App S3: this is currently done manually. App S3 needs just one extra file <code>HRC_20220328T0000-metadata.yml</code>: it is derived from <code>HRC_20220328T0000-files.yml</code>.</p> <pre><code>debates\n\u2514\u2500\u2500 HRC_20220328T0000\n    \u251c\u2500\u2500 HRC_20220328T0000-files.yml\n    \u251c\u2500\u2500 HRC_20220328T0000-metadata.yml\n    ...\n</code></pre> <p>Example for <code>HRC_20220328T0000-metadata.yml</code>:</p> <pre><code>s3_prefix: HRC_20160622T0000\nmedia:\n  key: HRC_20160622T0000.mp4\n  type: video\n  format: mp4\ns3_keys:\n  - name: HRC_20220328T0000.json\n    type: json\n    description: JSON file containing metadata transcription ...\n  - name: HRC_20220328T0000-files.yml\n    type: yml\n    description: YAML file containing metadata of the files ...\n  - name: HRC_20220328T0000.mp4\n    type: mp4\n    description: MP4 video file from the 2020 03 28 00:00 session\n  - name: HRC_20220328T0000-original.wav\n    type: wav\n    description: Original audio file from the 2020 03 28 00:00 session\n  - name: HRC_20220328T0000-transcription_original.srt\n    type: srt\n    description: Transcription file in SRT format ...\n  - name: HRC_20220328T0000-transcription_original.pdf\n    type: pdf\n    description: PDF file containing the transcription ...\n  - name: HRC_20220328T0000-translation_original_english.srt\n    type: srt\n    description: Translation file in SRT format to English ...\n  - name: HRC_20220328T0000-translation_original_english.pdf\n    type: pdf\n    description: PDF file containing the English translation ...\ncontext:\n  type: \"Human Rights Council\"\n  session: \"32th session\"\n  public: True\nschedule:\n  date: \"2016-06-22\"\n  time: \"10:00\"\n  timezone: \"Europe/Zurich\"\n</code></pre> <ul> <li>The metadata in <code>context</code> and <code>schedule</code> have been derived from   https://conf.unog.ch/digitalrecordings/en</li> <li><code>s3_prefix</code>: is the prefix or directory on S3, where the files for the media   item are stored</li> <li><code>media</code>: points to the actual media file that is played in the <code>media player</code>:</li> </ul> <p><code>media</code> subkeys:</p> <ul> <li><code>key</code>: is the actual media file</li> <li><code>type</code>: can be <code>video</code>or <code>audio</code></li> <li><code>format</code>: format is the media file format: for videos <code>mp4</code> is supported and   for audio files <code>wav</code>.</li> </ul>"
    },
    {
      "location": "dataloader.html#loading-into-mongodb-and-solr",
      "title": "Loading into Mongodb and Solr",
      "text": "<p>Warning</p> <p>Only do these steps on a fresh set up when you database is empty: otherwise it will mess up your existing data</p> <p>Once the environment is setup, the commands to load data into the mongodb and Solr are the following per media item:</p> <p>Then load the data from App S3:</p> <pre><code>python debates.py s3-to-mongo-solr HRC_20220328T0000\n</code></pre>"
    },
    {
      "location": "dataloader.html#start-api-server",
      "title": "Start API Server",
      "text": "<p>After this step the data should be available. You can now start the backend:</p> <pre><code>python debates.py serve\n</code></pre> <p>You will find the api documentation at <code>http://localhost:8000/docs</code> or as json file at <code>http://localhost:8000/openapi.json</code> For your convenience it has also been added to this documentation: api documentation</p>"
    },
    {
      "location": "processing.html",
      "title": "Processing Pipeline",
      "text": "<p>Note</p> <p>The pipeline can and has been run so far on a private GPU server</p> <p>The processing pipeline:</p> <pre><code>graph TB\n    ExternalUNDatabase[(External UN Database)] --&gt; UNScrapper[ODTP UN Scrapper]\n    subgraph ODTP\n    direction TB\n        UNScrapper --&gt; ODTPPyannoteWhisper[Diarization, Transcription &amp; Translation]\n    end\n    ODTPTranslation --&gt; UNS3[(Pipeline S3)]</code></pre> <p>Existing components:</p> <ul> <li>ODTP UN Scrapper <code>odtp-unog-digitalrecordings-scrapper</code>\u00a0\u29c9.   Component to scrap and download metadata from the UNOG Digital Recordings   platform.</li> <li>ODTP Pipeline <code>dt-political-debates</code>\u00a0\u29c9.   Repository compatible with ODTP and able to run the full pipeline</li> <li>ODTP Pyannote Whisper <code>odtp-pyannote-whisper</code>\u00a0\u29c9.   Component to diarize and transcribe audios and videos</li> </ul>"
    },
    {
      "location": "processing.html#how-to-run-the-pipeline-in-odtp",
      "title": "How to run the pipeline in ODTP?",
      "text": "<p>The easiest way to run the pipeline is to clone the ODTP Pipeline repository and run it in your ODTP instance following these instructions.</p> <p>You would need to configure all the parameters, variables, and secrets before doing so. The pipeline will fetch data from the UN Digital Recordings platform and perform transcriptions and translations over all the files in a date range.</p> <p>Please be aware that the use of the UN-Digital-Scrapper requires authorisation and an agreement with the UN. This component has been develop under an academic project for educational purposes.</p>"
    },
    {
      "location": "processing.html#how-to-run-the-pyannote-whisper-component",
      "title": "How to run the Pyannote Whisper component?",
      "text": "<p>If you wish just to run the Pyannote Whisper component, you can do it by deploying the service in your computer or cloning our Huggingface Space\u00a0\u29c9.</p> <p></p> <p>This space is running on a CPU which will make the transcription run slow. However, you can clone this space and request the use of GPUs. First you need to create an account in Huggingface.co, clone the space, configure the secrets, and select a GPU-based hardware.</p> <p>This pipeline processes a <code>.wav</code> or <code>mp4</code> media file by detecting the number of speakers present in the recording using <code>pyannote.audio</code>. For each detected speaker segment, it employs <code>OpenAI's Whisper model</code> to transcribe or translate the speech individually. This approach ensures accurate and speaker-specific transcriptions or translations, providing a clear understanding of who said what throughout the audio.</p> <p>Note: This application utilizes <code>pyannote.audio</code> and OpenAI's Whisper model. You must accept the terms of use on Hugging Face for the <code>pyannote/segmentation</code> and <code>pyannote/speaker-diarization</code> models before using this application.</p> <ul> <li>Speaker-Diarization\u00a0\u29c9</li> <li>Speaker-Segmentation\u00a0\u29c9</li> </ul> <p>After accepting these terms and conditions for those models. You can obtain you HuggingFace API Key to allow the access to these models:</p> <ul> <li>Hugging Face Access Keys\u00a0\u29c9</li> </ul> <p>This token should be provided to the component via the <code>ENV</code> variables or by the corresponding text field in the web app interface (Here\u00a0\u29c9).</p> <p>For more information check the README file on <code>odtp-pyannote-whisper</code>\u00a0\u29c9</p>"
    },
    { "location": "processing.html#outputs", "title": "Outputs", "text": "" },
    {
      "location": "processing.html#outputs-in-s3",
      "title": "Outputs in S3",
      "text": "<p>The S3 of the Pipeline contains the results for each media file processing: the results are structured in the following way:</p> <pre><code>debates\n\u2514\u2500\u2500 HRC_20220328T0000\n    \u251c\u2500\u2500 HRC_20220328T0000-files.yml\n    \u251c\u2500\u2500 HRC_20220328T0000-original.wav\n    \u251c\u2500\u2500 HRC_20220328T0000-transcription_original.json\n    \u251c\u2500\u2500 HRC_20220328T0000-transcription_original.pdf\n    \u251c\u2500\u2500 HRC_20220328T0000-transcription_original.srt\n    \u251c\u2500\u2500 HRC_20220328T0000-translation_original_english.json\n    \u251c\u2500\u2500 HRC_20220328T0000-translation_original_english.pdf\n    \u251c\u2500\u2500 HRC_20220328T0000-translation_original_english.srt\n    \u251c\u2500\u2500 HRC_20220328T0000.json\n    \u2514\u2500\u2500 HRC_20220328T0000.mp4\n</code></pre> <p>The operational important outputs are highlighted above and described below:</p> <ul> <li><code>debates</code>: is the S3 bucket for all outputs</li> <li><code>HRC_20220328T10000.mp4</code>: is the original media file that was processed: a   prefix is derived from the name <code>HRC_20220328T10000</code>. All outputs belonging to   the media file are stored under that prefix in the S3</li> <li><code>HRC_20220328T0000-files.yml</code>: contains all files with descriptions</li> <li><code>HRC_20220328T0000-transcription_original.srt</code>: the SRT file with the   transcription</li> <li><code>HRC_20220328T0000-translation_original_english.srt</code> the SRT file with the   translation. In this case the original audio has been translated to english.</li> <li><code>HRC_20220328T0000.mp4</code>: the media file that is played in the AppUI   videoplayer</li> </ul>"
    },
    {
      "location": "processing.html#json-file-format",
      "title": "JSON File Format",
      "text": "<p>This project has developped a JSON file format that combines metadata with the different transcriptions, translations, and other annotations. This single file. This file includes transcription at a sentence level and facilitates further analysis in Python, R or any other data analysis tool.</p> <p>The schema for this file is located here\u00a0\u29c9</p> <p>One minimal example will looks like this:</p> <pre><code>{\n  \"$schema\": \"https://github.com/sdsc-ordes/dt-political-debates/blob/main/schemas/sampleSchema.json\",\n  \"version\": \"1.0\",\n  \"metadata\": {\n    \"title\": \"Annual HPC User Meeting - 2023-05-15 09:30\",\n    \"date\": \"2023-05-15\",\n    \"time\": \"09:30\",\n    \"url\": \"https://meetings.sdsc.edu/hpcuser2023/downloads/session_recordings/2023-05-15_session1.zip\",\n    \"tags\": [\"SD230515A\"],\n    \"summary\": \"\",\n    \"labels\": {\n      \"Title\": \"Annual HPC User Meeting\",\n      \"RoomNumber\": \"Auditorium B\",\n      \"TimeFrom\": \"2023-05-15 09:30\",\n      \"TimeTo\": \"2023-05-15 12:00\",\n      \"RecordingStart\": \"09:32\",\n      \"UniqueNumber\": \"SD230515A\",\n      \"ClientName\": \"SDSC Computational Sciences\",\n      \"ClientCode\": \"90.1011\",\n      \"PrivPubl\": \"public\",\n      \"Exists\": \"true\",\n      \"sessionfile\": \"2023-05-15_09h32.txs\",\n      \"recorded\": true,\n      \"startdate\": \"2023-05-15\",\n      \"stopdate\": \"2023-05-15\",\n      \"starttime\": \"09:32:45\",\n      \"stoptime\": \"12:02:30\",\n      \"recordingtime\": \"02:29:45\",\n      \"archivefile\": \"2023-05-15_09h32.zip\",\n      \"clientname\": \"CSE Department\",\n      \"meetingNR\": \"5\",\n      \"privatepublic\": \"Public\",\n      \"summaryrecords\": \"True\",\n      \"sessiontitle\": \"Annual HPC User Group Meeting\",\n      \"lastchunk\": \"1\"\n    }\n  },\n  \"channels\": [\n    {\n      \"id\": \"video\",\n      \"type\": \"video\",\n      \"name\": \"Primary Video Feed\",\n      \"data\": \"SDSC_20230515_0930.mp4\",\n      \"tags\": [\"main\", \"video\"]\n    },\n    {\n      \"id\": \"original\",\n      \"type\": \"audio\",\n      \"name\": \"Raw Audio Stream\",\n      \"data\": \"SDSC_20230515_0930-original.wav\",\n      \"tags\": [\"original\", \"audio\"]\n    }\n  ],\n  \"annotations\": [\n    {\n      \"id\": \"markers_annotation\",\n      \"type\": \"markers\",\n      \"origin\": \"original\",\n      \"labels\": {\n        \"source\": \"markers\"\n      },\n      \"items\": [\n        {\n          \"id\": \"1\",\n          \"type\": \"TReX\",\n          \"origin\": \"original\",\n          \"labels\": {\n            \"type\": \"TReX\",\n            \"name\": \"Recording Start\",\n            \"info\": \"\"\n          },\n          \"start_timestamp\": \"09:32:45\",\n          \"end_timestamp\": \"09:32:45\"\n        },\n        {\n          \"id\": \"2\",\n          \"type\": \"User\",\n          \"origin\": \"original\",\n          \"labels\": {\n            \"type\": \"User\",\n            \"name\": \"MODERATOR\",\n            \"info\": \"Opening Remarks\"\n          },\n          \"start_timestamp\": \"09:32:50\",\n          \"end_timestamp\": \"09:34:00\"\n        }\n      ]\n    },\n    {\n      \"id\": \"transcription_original\",\n      \"type\": \"audio_transcription\",\n      \"originChannel\": \"original\",\n      \"labels\": {},\n      \"items\": [\n        {\n          \"transcript\": \"Good morning, everyone. Welcome to our annual HPC user group meeting.\",\n          \"start_timestamp\": \"00:00:05\",\n          \"end_timestamp\": \"00:00:10\",\n          \"labels\": {\n            \"speaker\": \"SPEAKER_01\",\n            \"language\": \"en\"\n          },\n          \"tags\": []\n        },\n        {\n          \"transcript\": \"We will start today's session with updates on recent system upgrades.\",\n          \"start_timestamp\": \"00:00:12\",\n          \"end_timestamp\": \"00:00:17\",\n          \"labels\": {\n            \"speaker\": \"SPEAKER_01\",\n            \"language\": \"en\"\n          },\n          \"tags\": []\n        }\n      ]\n    },\n    {\n      \"id\": \"translation_original_spanish\",\n      \"type\": \"audio_translation\",\n      \"originChannel\": \"original\",\n      \"labels\": {},\n      \"items\": [\n        {\n          \"transcript\": \"Buenos d\u00edas a todos. Bienvenidos a nuestra reuni\u00f3n anual del grupo de usuarios de HPC.\",\n          \"start_timestamp\": \"00:00:05\",\n          \"end_timestamp\": \"00:00:10\",\n          \"labels\": {\n            \"speaker\": \"SPEAKER_01\",\n            \"language\": \"es\"\n          },\n          \"tags\": []\n        },\n        {\n          \"transcript\": \"Comenzaremos la sesi\u00f3n de hoy con actualizaciones sobre mejoras recientes en los sistemas.\",\n          \"start_timestamp\": \"00:00:12\",\n          \"end_timestamp\": \"00:00:17\",\n          \"labels\": {\n            \"speaker\": \"SPEAKER_01\",\n            \"language\": \"es\"\n          },\n          \"tags\": []\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"
    },
    {
      "location": "processing.html#srt-files",
      "title": "SRT files",
      "text": "<p>SRT is a commonly known Standard for subtitles. Below your see an example of an SRT file. The SRT files that the pipeline outputs already contain speaker information in form of a speaker IDs such as <code>SPEAKER_06</code>. The speaker_ids are derived by diarization.</p> <pre><code>1\n00:00:00,031 --&gt; 00:00:17,811\n[SPEAKER_06]:  Pasamos inmediatamente al tema 9, ...\n\n2\n00:00:17,811 --&gt; 00:00:31,451\n[SPEAKER_06]:  Procedemos ahora a la presentaci\u00f3n ...\n\n3\n00:00:31,451 --&gt; 00:00:47,751\n[SPEAKER_06]:  Tengo el placer de dar la bienvenida ...\n\n4\n00:00:47,811 --&gt; 00:00:50,631\n[SPEAKER_06]:  Excelencia, querida embajadora, tiene la palabra.\n\n5\n00:00:55,145 --&gt; 00:00:57,305\n[SPEAKER_00]:  Thank you, Chair Passon.\n\n6\n00:00:57,305 --&gt; 00:01:00,525\n[SPEAKER_00]:  Excellencies, ladies and gentlemen,\n</code></pre>"
    },
    {
      "location": "processing.html#additonal-outputs",
      "title": "Additonal outputs",
      "text": "<p>Example of <code>HRC_20220328T0000-files.yml</code></p> <pre><code>files:\n  - name: HRC_20220328T0000.json\n    type: json\n    description: JSON file containing metadata transcription ...\n  - name: HRC_20220328T0000-files.yml\n    type: yml\n    description: YAML file containing metadata of the files ...\n  - name: HRC_20220328T0000.mp4\n    type: mp4\n    description: MP4 video file from the 2020 03 28 00:00 session\n  - name: HRC_20220328T0000-original.wav\n    type: wav\n    description: Original audio file from the 2020 03 28 00:00 session\n  - name: HRC_20220328T0000-transcription_original.srt\n    type: srt\n    description: Transcription file in SRT format ...\n  - name: HRC_20220328T0000-transcription_original.pdf\n    type: pdf\n    description: PDF file containing the transcription ...\n  - name: HRC_20220328T0000-translation_original_english.srt\n    type: srt\n    description: Translation file in SRT format to English ...\n  - name: HRC_20220328T0000-translation_original_english.pdf\n    type: pdf\n    description: PDF file containing the English translation ...\n</code></pre> <p>After the media has been processed it is loaded into the Debates App via the dataloader.</p>"
    },
    { "location": "status.html", "title": "PoC Status", "text": "" },
    {
      "location": "status.html#this-is-a-poc-and-not-meant-for-production-usage",
      "title": "This is a PoC and not meant for production usage",
      "text": "<ul> <li>The data is stored on docker volumes</li> <li>there is no data backup and recovery mechanism in place</li> <li>basic authentication has been added to allow for just two users with roles reader and editor</li> </ul>"
    },
    {
      "location": "status.html#possible-improvements",
      "title": "Possible improvements",
      "text": ""
    },
    {
      "location": "status.html#data-management",
      "title": "Data management",
      "text": "<p>Instead of using docker volumes, proper databases can be added, see server setup</p>"
    },
    {
      "location": "status.html#authentication",
      "title": "Authentication",
      "text": "<p>It would make sense to add a proper authentication, distinguishing between users with the role <code>editor</code> and allowing them to upload private projects, where only they can edit the metadata.</p>"
    },
    { "location": "webui.html", "title": "WebUI", "text": "" },
    {
      "location": "webui.html#architecture",
      "title": "Architecture",
      "text": "<ul> <li>The User Interface talks to the Backend via API. See   API Documentation</li> <li>The Web Ui is build with Sveltekit\u00a0\u29c9</li> </ul> <pre><code>graph TB\n    subgraph Webplatform\n    direction TB\n        A[Political Debates Backend]\n        B[Political Debates UI]\n        C[(Political Debates MongoDB)]\n        D[(Political Debates Solr)]\n        E[(Political Debates S3)]\n        A -- annotate --&gt; C\n        A -- search --&gt; D\n        A -- signed urls --&gt; E\n        B -- api --&gt; A\n    end</code></pre>"
    },
    {
      "location": "webui.html#user-roles",
      "title": "User Roles",
      "text": "<p>Currently the User interface has no Authentication added. In a Server installation we installed two password protected users: (see <code>nginx</code> folder)</p> <ul> <li><code>reader</code>: can only view the page</li> <li><code>editor</code>: can add annotation on the media player page</li> </ul> <p>You also have these roles in other installation mode, by just going on the routes:</p> <ul> <li><code>/edit</code>: is the route for the <code>editor</code> role</li> <li><code>/</code>: switches to the reader role.</li> </ul>"
    },
    {
      "location": "webui.html#homepage-editor",
      "title": "Homepage Editor",
      "text": "<p>{ width=\"800\", caption=\"hello\" } /// caption Homepage of the Editor User available at <code>/edit</code> ///</p>"
    },
    {
      "location": "webui.html#homepage-reader",
      "title": "Homepage Reader",
      "text": "<p>{ width=\"800\", caption=\"hello\" } /// caption Homepage of the Reader User available at <code>/</code> ///</p>"
    },
    {
      "location": "webui.html#search-page",
      "title": "Search Page",
      "text": "<p>{ width=\"800\", caption=\"hello\" } /// caption Search interface to make speaker statements searchable available at <code>/search</code> ///</p>"
    },
    {
      "location": "webui.html#mediaplayer-page-as-editor",
      "title": "Mediaplayer Page as Editor",
      "text": "<p>{ width=\"800\", caption=\"hello\" } /// caption Media player detail page for the reader user to compare transcripts and translations ///</p>"
    },
    {
      "location": "webui.html#mediaplayer-page-as-reader",
      "title": "Mediaplayer Page as Reader",
      "text": "<p> /// caption Media player detail page for the Editor user: the editor user can correct the transcripts and translations and add speaker name and Role ///</p>"
    },
    {
      "location": "api/api.html",
      "title": "API Documentation",
      "text": "<p>The API can be seen as a second interface to processing and data along to the UI</p>"
    },
    {
      "location": "api/api.html#ingestion",
      "title": "Ingestion",
      "text": "<pre><code>graph TD\n    subgraph Users [\"Users\"]\n        User((Editor))\n    end\n\n    subgraph Clients [\"User Interaction\"]\n        FE[Frontend&lt;br&gt;SvelteKit]\n    end\n\n    subgraph App_Layer [\"Application Core\"]\n        BE[Backend&lt;br&gt;Python FastAPI]\n\n        %% logical steps inside backend\n        GetUrl[Get Presigned Post]\n        Process[Start Processing]\n    end\n\n    subgraph Async_Layer [\"Asynchronous Processing: Workers\"]\n        Redis[Redis Queue]\n        Converter[\"Convert Worker\"]\n        Transcriber[\"Transcribe Worker\"]\n    end\n\n    subgraph Storage1 [\"Primary Data Storage\"]\n        S3[S3 Storage&lt;br&gt;Media &amp; Artifacts]\n    end\n\n    %% --- Relationships ---\n\n    %% User Action\n    User --&gt;|Uploads media| FE\n\n    %% 1. Upload Flow\n    FE --&gt;|1. Request Upload URL| GetUrl\n    GetUrl -- Sign URL --&gt; S3\n    GetUrl --&gt;|Return URL| FE\n    FE -.-&gt;|2. Direct Upload| S3\n\n    %% 2. Processing Flow\n    FE --&gt;|3. Trigger Processing| Process\n    Process --&gt;|Enqueue Job| Redis\n\n    %% 3. Async Execution\n    Redis --&gt;|Dequeue Video Job| Converter\n    Redis --&gt;|Dequeue Audio Job| Transcriber\n\n    %% 4. Worker Actions\n    Converter --&gt;|Save Extracted Audio| S3\n\n    %% Transcriber reads the audio converted by the previous step\n    S3 -.-&gt;|Read Audio| Transcriber\n    Transcriber --&gt;|Save Transcription| S3\n\n    %% --- Styling ---\n    classDef primary fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n    classDef secondary fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;\n    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px;\n    classDef external fill:#fff3e0,stroke:#e65100,stroke-width:2px,stroke-dasharray: 5 5;\n    classDef client fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;\n    classDef user fill:#ffccbc,stroke:#d84315,stroke-width:2px;\n\n    class FE client;\n    class BE,GetUrl,Process primary;\n    class Redis,Converter,Transcriber secondary;\n    class S3 storage;\n    class User user;</code></pre> <ul> <li><code>ingest/get_presinged-post</code>: gets a presigned post url, that can be used to   upload the media file on S3</li> <li><code>ingest/process</code>: triggers the first job on redis queue: either a <code>convert</code> or   a <code>trnascribe</code> task, depending on the type of the media.</li> </ul> <p>See Frontend</p>"
    },
    {
      "location": "api/api.html#metadata",
      "title": "Metadata",
      "text": "<p>--8&lt;-- \"includes/diagrams/metadata-display.md\" --8&lt;-- \"includes/diagrams/metadata-update.md\"</p> <ul> <li><code>ingest/get_presinged-post</code>: gets a presigned post url, that can be used to   upload the media file on S3</li> <li><code>ingest/process</code>: triggers the first job on redis queue: either a <code>convert</code> or   a <code>trnascribe</code> task, depending on the type of the media.</li> </ul> <p>See Frontend</p>"
    },
    {
      "location": "api/api.html#search",
      "title": "Search",
      "text": "<pre><code>graph TD\n    subgraph Users [\"Users\"]\n        User((Reader, Editor))\n    end\n\n    subgraph Clients [\"User Interaction\"]\n        FE[Frontend&lt;br&gt;SvelteKit]\n    end\n\n    subgraph App_Layer [\"Application Core\"]\n        BE[Backend&lt;br&gt;Python FastAPI]\n\n        %% logical steps inside backend\n        Search[Search Solr]\n    end\n\n    subgraph Storage2 [\"Secondary Datastorage\"]\n        Solr[Solr Search]\n    end\n\n    %% --- Relationships ---\n\n    %% User Action\n    User --&gt;|Searches on| FE\n\n    %% Search Flow\n    FE --&gt;|Search by query term and filters| Search\n    Search -- Get segments --&gt; Solr\n\n    %% --- Styling ---\n    classDef primary fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n    classDef secondary fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;\n    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px;\n    classDef external fill:#fff3e0,stroke:#e65100,stroke-width:2px,stroke-dasharray: 5 5;\n    classDef client fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;\n    classDef user fill:#ffccbc,stroke:#d84315,stroke-width:2px;\n\n    class FE client;\n    class BE,Search primary;\n    class Solr storage;\n    class User user;</code></pre> <ul> <li><code>ingest/get_presinged-post</code>: gets a presigned post url, that can be used to   upload the media file on S3</li> <li><code>ingest/process</code>: triggers the first job on redis queue: either a <code>convert</code> or   a <code>trnascribe</code> task, depending on the type of the media.</li> </ul> <p>See Frontend</p>"
    },
    {
      "location": "api/api.html#admin",
      "title": "Admin",
      "text": "<pre><code>graph TD\n    subgraph Users [\"Users\"]\n        User((Editor))\n    end\n\n    subgraph Clients [\"User Interaction: Editor\"]\n        FE[Frontend&lt;br&gt;SvelteKit]\n    end\n\n    subgraph App_Layer [\"Application Core\"]\n        BE[Backend&lt;br&gt;Python FastAPI]\n\n        %% logical steps inside backend\n        List[List media]\n        Delete[Delete media by id]\n        Reindex[Reindex media by id]\n    end\n\n    subgraph Storage1 [\"Primary Data Storage\"]\n        S3[S3 Storage&lt;br&gt;Media &amp; Artifacts]\n    end\n\n    subgraph Storage2 [\"Secondary Datastorage\"]\n        Mongo[MongoDB]\n        Solr[Solr Search]\n    end\n\n\n    %% --- Relationships ---\n\n    %% User Action\n    User --&gt;|Administers uploaded media| FE\n\n    %% List Flow\n    FE --&gt;|List all media | List\n    List -- Get all --&gt; Mongo\n    List -- Get all --&gt; Solr\n    List -- Get all --&gt; S3\n\n    %% Reindex Flow\n    FE --&gt;|Reindex by media_id | Reindex\n    Reindex -- 1. Get data for media_id --&gt; S3\n    Reindex -- 2. Update media_id --&gt; Mongo\n    Reindex -- 2. Update media_id --&gt; Solr\n\n    %% Delete Flow\n    FE --&gt;|Delete by media_id | Delete\n    Delete -- Delete media_id --&gt; Mongo\n    Delete -- Delete media_id --&gt; Solr\n    Delete -- Delete media_id --&gt; S3\n\n    %% --- Styling ---\n    classDef primary fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n    classDef secondary fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;\n    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px;\n    classDef external fill:#fff3e0,stroke:#e65100,stroke-width:2px,stroke-dasharray: 5 5;\n    classDef client fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;\n    classDef user fill:#ffccbc,stroke:#d84315,stroke-width:2px;\n\n    class FE client;\n    class BE,List,Reindex,Delete primary;\n    class S3,Mongo,Solr storage;\n    class User user;</code></pre> <ul> <li><code>ingest/get_presinged-post</code>: gets a presigned post url, that can be used to   upload the media file on S3</li> <li><code>ingest/process</code>: triggers the first job on redis queue: either a <code>convert</code> or   a <code>trnascribe</code> task, depending on the type of the media.</li> </ul> <p>See Frontend</p>"
    },
    { "location": "api/api.html#api-spec", "title": "API Spec", "text": "" },
    {
      "location": "install/compose.html",
      "title": "Compose setup",
      "text": ""
    },
    {
      "location": "install/compose.html#clone-the-repo",
      "title": "Clone the repo:",
      "text": "<pre><code>git clone git@github.com:sdsc-ordes/debates-analytics.git\ncd debates-analytics\n</code></pre> <p>The repository uses nix:</p>"
    },
    {
      "location": "install/compose.html#nix-setup",
      "title": "Nix setup",
      "text": ""
    },
    {
      "location": "install/compose.html#environment-variables-for-debates-app",
      "title": "Environment variables for Debates App",
      "text": "<pre><code>cd deploy\ncp .env.tmpl .env\ncp .env.secret.tmpl .env.secret\n</code></pre> <p>You need to set the following credentials:</p> <ul> <li>App S3: <code>[ACCESS_KEY]</code>, <code>[SECRET_KEY]</code></li> <li>Mongo DB: <code>[MONGO_USER]</code>, <code>[MONGO_PASSWORD]</code></li> <li>Mongo Express: <code>[MONGO_EXPRESS_USER]</code>, <code>[MONGO_EXPRESS_PASSWORD]</code></li> </ul> <p>You also need a directory for volumes:</p> <pre><code>debates\n\u251c\u2500\u2500 debates-app\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 minio\n    \u251c\u2500\u2500 mongo\n    \u2514\u2500\u2500 s3\n</code></pre> <p>Save the pathes:</p> <ul> <li><code>[PATH_MINIO]</code></li> <li><code>[PATH_MONGO]</code></li> <li><code>[PATH_S3]</code></li> </ul> <p>Then the <code>.env</code> file is filled as follow:</p> <pre><code># Credentials\n# s3\nS3_ACCESS_KEY=ACCESS_KEY]\nS3_SECRET_KEY=[SECRET_KEY]\n# Mongo DB\nMONGO_USER=[MONGO_USER]\nMONGO_PASSWORD=[MONGO_PASSWORD]\n# Mongo Express\nMONGO_EXPRESS_USER=[MONGO_EXPRESS_USER]\nMONGO_EXPRESS_PASSWORD=[MONGO_EXPRESS_PASSWORD]\n\n# volumes to store the data of services in docker compose\nSOLR_PATH=[PATH_SOLR]\nMONGO_PATH=[PATH_MONGO]\nMINIO_PATH=[PATH_MINIO]\n</code></pre>"
    },
    {
      "location": "install/compose.html#environment-variables-for-frontend",
      "title": "Environment Variables for frontend",
      "text": "<p>The frontend is build with Sveltekit\u00a0\u29c9 and also a environment variable:</p> <pre><code>cd frontend\nsp .env.dist .env\n</code></pre> <p>The <code>.env</code> file of the frontend should look like this:</p> <pre><code># for docker compose setup\nPUBLIC_BACKEND_SERVER=http://dataloader:8000\n</code></pre> <p>The url connects it to the backend.</p>"
    },
    {
      "location": "install/compose.html#build-docker-compose",
      "title": "Build docker compose",
      "text": "<p>Now you are ready to build and run docker compose</p> <pre><code>docker compose build\ndocker compose -f docker-compose.compose up -d\n</code></pre> <p>Now that the containers are running, enter the dataloader container</p>"
    },
    {
      "location": "install/compose.html#load-data",
      "title": "Load data",
      "text": "<p>This step describes the initial data loading</p> <pre><code>docker exec -it debates-dataloader-1 sh\n</code></pre>"
    },
    {
      "location": "install/development.html",
      "title": "Development setup",
      "text": "<p>In the development setup the docker compose file <code>docker-compose.dev.yml</code> is used to run only the databases as services. The dataloader and frontend are then run locally outside of docker.</p> <pre><code>git clone git@github.com:sdsc-ordes/debates-app.git\ncd debates-app\ngit submodule update --init\n</code></pre>"
    },
    {
      "location": "install/development.html#environment-variables-for-docker-compose",
      "title": "Environment Variables for docker compose",
      "text": "<p>The env variables are similar to the compose setup: you have to set:</p> <ul> <li>environment for the debates app</li> </ul>"
    },
    {
      "location": "install/development.html#dev-setup-of-docker-compose",
      "title": "Dev setup of docker compose",
      "text": "<p>The development setup has it's own docker compose file <code>docker-compose.dev.yml</code>.</p> <pre><code>docker compose -f docker-compose.dev.yml build\ndocker compose -f docker-compose.dev.yml up -d\n</code></pre>"
    },
    {
      "location": "install/development.html#dev-setup-of-the-dataloader-backend",
      "title": "Dev setup of the dataloader backend",
      "text": "<p>The backend needs now its own <code>backend.env</code> as follows:</p> <pre><code># External Service settings:\n# must match the docker compose settings for the services\nSOLR_URL=http://localhost:8010/solr/debates/\nS3_ACCESS_KEY=\nS3_SECRET_KEY=\n\n# Use this for local\nAPI_HOST=\"127.0.0.1\"\nMONGO_URL=\nS3_SERVER=http://localhost:9000\nFRONTEND_SERVER=http://localhost:5173\n\n# Use this for compose\nS3_FRONTEND_BASE_URL=\n\n# this is for the PROD S3\nPROD_S3_BUCKET_NAME=\nPROD_S3_ACCESS_KEY=\nPROD_S3_SECRET_KEY=\nPROD_S3_REGION_NAME=\n</code></pre> <p>The <code>PROD_S3_*</code> are there for the transfer of data from the Pipeline S3 to the App S3.</p> <p>After you have loaded the data into the App S3, you can build the backend and then serve the backend api:</p> <pre><code>cd dataloader\nrye install\nrye sync\nsource .venv/bin/activate\npython src/debates.py serve\n</code></pre>"
    },
    {
      "location": "install/development.html#dev-setup-of-the-frontend",
      "title": "Dev setup of the frontend",
      "text": "<p>You need to adapt the <code>frontend/.env</code>:</p> <pre><code># in local dev setup\n#PUBLIC_BACKEND_SERVER=http://0.0.0.0:8000\n</code></pre> <p>After that install and mount the frontend with:</p> <pre><code>cd frontend\npnpm install\npnpm dev\n</code></pre> <p>The frontend should now be up and running and tell you the url where it is available</p>"
    },
    {
      "location": "install/development.html#load-data",
      "title": "Load data",
      "text": "<p>See compose setup</p>"
    },
    {
      "location": "install/development.html#dev-setup-of-the-documentation",
      "title": "Dev setup of the documentation",
      "text": "<p>The docs are powered by uv\u00a0\u29c9 as python package manager and Material for Mkdocs\u00a0\u29c9 as documentation package</p> <pre><code>cd docs\nuv build\nuv venv\nsource .venv/bin/activate\nuv sync\nmkdocs serve\n</code></pre>"
    },
    {
      "location": "install/nix.html",
      "title": "Nix setup",
      "text": "<p>To setup nix ...</p>"
    },
    {
      "location": "install/options.html",
      "title": "Installation overview",
      "text": "<p>The debates app can be installed in 2 different ways:</p> <ul> <li>on a local computer with docker-compose compose setup</li> <li>on a server for production usage: see production setup</li> <li>for development, see development setup</li> </ul>"
    },
    {
      "location": "install/server.html",
      "title": "Server setup",
      "text": "<p>The setup on a server is similar to the compose setup. It will not described in detail here.</p>"
    },
    {
      "location": "install/server.html#environment-variables",
      "title": "Environment Variables",
      "text": "<p>The env variables are similar to the compose setup: you have to set:</p> <ul> <li>environment for the debates app</li> <li>environment for the frontend</li> </ul>"
    },
    {
      "location": "install/server.html#docker-compose",
      "title": "Docker compose",
      "text": "<p>The server setup has it's own docker compose file <code>docker-compose.yml</code>. The docker compose commands will use that file, but you can also specify it with <code>-f docker-compose.yml</code></p> <pre><code>docker compose build\ndocker compose up -d\n</code></pre>"
    },
    {
      "location": "install/server.html#proxy-server",
      "title": "Proxy Server",
      "text": "<p>Their is an additional reverse proxy that has the following tasks:</p> <ul> <li>password protect access to the debates app for all users</li> <li>restrict access to the route <code>edit</code> to the <code>editor user</code></li> <li>provide lets encrypt certification to make the debates api available under   <code>https</code></li> </ul>"
    },
    {
      "location": "install/server.html#load-data",
      "title": "Load data",
      "text": "<p>See compose setup</p>"
    },
    {
      "location": "ui/ingest.html",
      "title": "Ingestion",
      "text": "<pre><code>graph TD\n    subgraph Users [\"Users\"]\n        User((Editor))\n    end\n\n    subgraph Clients [\"User Interaction\"]\n        FE[Frontend&lt;br&gt;SvelteKit]\n    end\n\n    subgraph App_Layer [\"Application Core\"]\n        BE[Backend&lt;br&gt;Python FastAPI]\n\n        %% logical steps inside backend\n        GetUrl[Get Presigned Post]\n        Process[Start Processing]\n    end\n\n    subgraph Async_Layer [\"Asynchronous Processing: Workers\"]\n        Redis[Redis Queue]\n        Converter[\"Convert Worker\"]\n        Transcriber[\"Transcribe Worker\"]\n    end\n\n    subgraph Storage1 [\"Primary Data Storage\"]\n        S3[S3 Storage&lt;br&gt;Media &amp; Artifacts]\n    end\n\n    %% --- Relationships ---\n\n    %% User Action\n    User --&gt;|Uploads media| FE\n\n    %% 1. Upload Flow\n    FE --&gt;|1. Request Upload URL| GetUrl\n    GetUrl -- Sign URL --&gt; S3\n    GetUrl --&gt;|Return URL| FE\n    FE -.-&gt;|2. Direct Upload| S3\n\n    %% 2. Processing Flow\n    FE --&gt;|3. Trigger Processing| Process\n    Process --&gt;|Enqueue Job| Redis\n\n    %% 3. Async Execution\n    Redis --&gt;|Dequeue Video Job| Converter\n    Redis --&gt;|Dequeue Audio Job| Transcriber\n\n    %% 4. Worker Actions\n    Converter --&gt;|Save Extracted Audio| S3\n\n    %% Transcriber reads the audio converted by the previous step\n    S3 -.-&gt;|Read Audio| Transcriber\n    Transcriber --&gt;|Save Transcription| S3\n\n    %% --- Styling ---\n    classDef primary fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n    classDef secondary fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;\n    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px;\n    classDef external fill:#fff3e0,stroke:#e65100,stroke-width:2px,stroke-dasharray: 5 5;\n    classDef client fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;\n    classDef user fill:#ffccbc,stroke:#d84315,stroke-width:2px;\n\n    class FE client;\n    class BE,GetUrl,Process primary;\n    class Redis,Converter,Transcriber secondary;\n    class S3 storage;\n    class User user;</code></pre> <ul> <li>uses ingest api [../api/api.md#ingest]</li> </ul>"
    }
  ]
}
